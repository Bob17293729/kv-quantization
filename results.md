(ssm_hw6) [cs601-asures13@gpuz01 kv-quantization]$ python generate.py --checkpoint_path checkpoints/Mistral-7B/model.pth --prompt "Once upon a time" --max_new_tokens 512
Using device=cuda
Loading model ...
[DEBUG] Model loaded successfully.
[DEBUG] Model config: ModelArgs(block_size=2048, vocab_size=32768, n_layer=32, n_head=32, dim=4096, intermediate_size=14336, n_local_heads=8, head_dim=128, rope_base=10000, norm_eps=1e-05, rope_scaling=None, has_qkv_bias=False)
[DEBUG] Model total parameters: 7248.02M
Time to load model: 4.25 seconds
[DEBUG] Tokenizer loaded successfully.
[DEBUG] Prompt encoded, shape: torch.Size([5]), first 10 tokens: [1, 6481, 4482, 1032, 1495]
[DEBUG] Entered generate()
[DEBUG] prompt shape: torch.Size([5])
[DEBUG] batch_size = 1, max_new_tokens = 512
[DEBUG] Calling prefill()
[DEBUG] Inside prefill()
[DEBUG] x.shape: torch.Size([1, 5]), input_pos.shape: torch.Size([5])
[DEBUG] After view(-1), input_pos.shape: torch.Size([5])
[DEBUG] Prefill completed, next token sampled.
[DEBUG] Starting decode_n_tokens() loop
Once upon a time I used to work from 8am to 6pm… I used to be so tired I literally used to fall asleep in the shower… which is why I always used to shower at night… It was a lot more convenient that way…

Anyways I used to pretty much spend all of my time at work… I would spend all of my time at work… so, therefore, all of my time during those days was spent at work…

So, when I got laid off from work and then, a few months later, I started working at McDonald’s as a cashier… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard… Well, you can’t really work at McDonald’s as a cashier if you don’t have a timecard…

Once upon a time I used to work at McDonald’s as a cashier… and, in order to get paid, I had to have a timecard… and, in order to get a timecard, I had to ask my manager for one… and, in order to get my manager to give me a timecard, I had to convince him that I was a trustworthy employee who would be able to use a timecard responsibly and in a way that would not result in any negative consequences for either myself or for McDonald’s…

So, given that I was a very trustworthy person who would always be able to be counted on to do the right thing and who would always be able to be counted on to do the right thing and who would always be able to be counted on to do the right thing and who would always be able to be counted on to do the right thing and who would always be able to be counted on to do the right thing and who would always be able to
Time for inference 1: 42.61 sec total, 12.02 tokens/sec
Bandwidth achieved: 170.96 GB/s
FLOPS achieved: 0.17 TF/s

[DEBUG] Entered generate()
[DEBUG] prompt shape: torch.Size([5])
[DEBUG] batch_size = 1, max_new_tokens = 512
[DEBUG] Calling prefill()
[DEBUG] Inside prefill()
[DEBUG] x.shape: torch.Size([1, 5]), input_pos.shape: torch.Size([5])
[DEBUG] After view(-1), input_pos.shape: torch.Size([5])
[DEBUG] Prefill completed, next token sampled.
[DEBUG] Starting decode_n_tokens() loop
Once upon a time, there lived a great, wise, and powerful monarch. He was a man who knew what he wanted out of life. His one dream was to one day rule the world. He was a man of great ambition.

The King had many servants at his beck and call. He had a swashbuckling sword master, a strong and sturdy stonecutter, a reliable and trustworthy tax collector, a brilliant and wise wizard, and a notorious and infamous highwayman.

The King was not a man to give up on his dreams. He knew that if he wanted to achieve his goal of becoming the ruler of the entire world, then he was going to have to put forth a tremendous amount of effort.

The King had a great love for his servants. He loved them as if they were his own family members. He loved them so much that he would often reward them by giving them gifts or by granting them special privileges.

One day, the King decided to reward his servants by giving them a special gift. He ordered his servants to meet him in the great hall of the castle. Once they had all arrived, the King addressed his servants saying, “My loyal and faithful servants, on today, this day, I have decided to reward you by giving you a special gift. I have chosen for you a gift that I believe will be of great benefit to you. I have decided to give to each of you a crystal ball. The crystal ball is a magical item that has the power to show you what the future holds. Whenever you want to know what lies ahead for you, all you have to do is pick up the crystal ball and look into it. The crystal ball will then show you what is in store for you in the future.

Now, there is just one small little caveat that I must warn you about before I give you your crystal ball. The crystal ball that I am giving to each of you is a very powerful and very magical item. It is a very potent tool that has the ability to show you what the future holds. The crystal ball is a very valuable and very precious item. Because of its great power and its great value, there is just one small little rule that I must ask you to obey before I give you your crystal ball.

The one small little rule that I am asking you to obey before I give you your crystal ball is that you must never, ever, under any
Time for inference 2: 38.43 sec total, 13.32 tokens/sec
Bandwidth achieved: 189.56 GB/s
FLOPS achieved: 0.19 TF/s

[DEBUG] Entered generate()
[DEBUG] prompt shape: torch.Size([5])
[DEBUG] batch_size = 1, max_new_tokens = 512
[DEBUG] Calling prefill()
[DEBUG] Inside prefill()
[DEBUG] x.shape: torch.Size([1, 5]), input_pos.shape: torch.Size([5])
[DEBUG] After view(-1), input_pos.shape: torch.Size([5])
[DEBUG] Prefill completed, next token sampled.
[DEBUG] Starting decode_n_tokens() loop
Once upon a time there was a man named David. He was a shepherd. He had a relationship with the sheep. He was close to each one individually. He knew what they were doing, who they were with, and what their needs were at any given point in time.

David also had another unique ability. He could easily leave the sheep behind and walk into the wilderness and fight a lion, or run after a bear, or capture a predator.

And when he returned back to the sheep, the sheep were grateful and they all gathered around his neck and kissed him.

The End

A.J. Castellitto

www.truthinsomnia.com ISBN 978-1-93519-4-1 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-1-93519-4-8 ISBN 978-1-93519-4-7 ISBN 978-1-93519-4-5 ISBN 978-
Time for inference 3: 38.38 sec total, 13.34 tokens/sec
Bandwidth achieved: 189.78 GB/s
FLOPS achieved: 0.19 TF/s

[DEBUG] Entered generate()
[DEBUG] prompt shape: torch.Size([5])
[DEBUG] batch_size = 1, max_new_tokens = 512
[DEBUG] Calling prefill()
[DEBUG] Inside prefill()
[DEBUG] x.shape: torch.Size([1, 5]), input_pos.shape: torch.Size([5])
[DEBUG] After view(-1), input_pos.shape: torch.Size([5])
[DEBUG] Prefill completed, next token sampled.
[DEBUG] Starting decode_n_tokens() loop
Once upon a time, there lived a little girl.

She was afraid.

She had just heard a story that involved dragons. They were huge and scary and they wanted to eat ALL THE PEOPLE.

She was afraid.

She was concerned that the story would come true. She didn’t want to live in fear, and she didn’t want to live in a world where dragons posed a real and present threat to people.

She looked and looked and looked and she couldn’t find any dragons to be afraid of.

She realized that she can’t be afraid of that which does not exist.

The dragons were imaginary. The dragon story was imaginary. The world in which the dragon story took place was imaginary.

There were no real life dragons.

There was no real world dragon story.

There was no real world where the dragon story took place.

The only dragon story, and the only world in which the dragon story took place, was in the tiny little girl’s mind.

She had never seen a real life dragon.

The only dragons that existed were the imaginary dragons that lived in the little girl’s mind.

She had created them.

They had been living inside of her head since the moment that she had first learned about them, and the moment that she first created them.

The dragons had been living inside of her head, and she had been living inside of the dragons heads, since the moment that she first created them.

She had never seen a real life dragon, but the dragons had often seen her. They had seen her since the moment that she first created them.

She had never seen a real life dragon, but the dragons had often seen her. They had seen her since the moment that she first created them.

The dragons were only one thing, and that one thing was the most important thing to them.

That one thing was the thing that they were most afraid of, and the thing that they were most hopeful about.

That one thing was the thing that they were most excited about, and the thing that they were most nervous about.

That one thing was the thing that they were most curious about, and the thing that they were most worried about.

That one thing was the thing that they were most doubtful about, and the thing that
Time for inference 4: 38.51 sec total, 13.30 tokens/sec
Bandwidth achieved: 189.18 GB/s
FLOPS achieved: 0.19 TF/s

[DEBUG] Entered generate()
[DEBUG] prompt shape: torch.Size([5])
[DEBUG] batch_size = 1, max_new_tokens = 512
[DEBUG] Calling prefill()
[DEBUG] Inside prefill()
[DEBUG] x.shape: torch.Size([1, 5]), input_pos.shape: torch.Size([5])
[DEBUG] After view(-1), input_pos.shape: torch.Size([5])
[DEBUG] Prefill completed, next token sampled.
[DEBUG] Starting decode_n_tokens() loop
Once upon a time, a young man named Adam came to work at Aspen Insurance.  He was young, eager and determined to prove his worth.

On his first day, Adam was assigned to a desk near the bustling telecommunications room.  Despite the noise, Adam was determined to do his best.

By the end of his first week, Adam had impressed his supervisors with his ability to make sense of complicated insurance policies.

As Adam continued to work hard, he soon found himself promoted to a more important position.

Over time, Adam’s hard work and dedication paid off.  He was promoted to management and was eventually chosen to be the president of the company.

Adam’s story is a classic example of how hard work and dedication can pay off.  Adam’s story is a reminder that if you work hard and stay dedicated to your goals, success will eventually come your way.мрдщшщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщщ
Time for inference 5: 38.25 sec total, 13.39 tokens/sec
Bandwidth achieved: 190.45 GB/s
FLOPS achieved: 0.19 TF/s

==========
Batch Size: 1
Prompt Length: 5
Generated tokens: 512
Average tokens/sec: 13.07
Memory used: 14.64 GB